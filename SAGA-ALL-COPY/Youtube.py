
# This file was generated by the Tkinter Designer by Parth Jadhav
# https://github.com/ParthJadhav/Tkinter-Designer


from pathlib import Path

import tkinter
# Explicit imports to satisfy Flake8
from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage, messagebox
from tkinter import *
from tkinter.ttk import Combobox
from PIL import ImageTk, Image
from tkcalendar import DateEntry
from collections import Counter
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure
from tkinter import filedialog
from langdetect import detect

import emoji
import nest_asyncio
import pandas
import emoji
import re
import numpy

from wordcloud import STOPWORDS, WordCloud


import tkinter.ttk as ttk
import csv
import sys

from csv import writer
from csv import reader
import json
import os
import time
import requests
import pandas as pd

nest_asyncio .apply() #hides the cmd result, good for speed of performance

OUTPUT_PATH = Path(__file__).parent
ASSETS_PATH = OUTPUT_PATH / Path("./assets")


def relative_to_assets(path: str) -> Path:
    return ASSETS_PATH / Path(path)

#code from youtube-downloader
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

YOUTUBE_COMMENTS_AJAX_URL = 'https://www.youtube.com/comment_service_ajax'

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'
# csv file name
FILE_NAME = 'yt_intialcomments.csv'

# set parameters
SORT_BY_POPULAR = 0 # filter comments by popularity or recent, 0:False, 1:True
SORT_BY_RECENT = 0 # default recent False, change to 1 to download latest comments


YT_CFG_RE = r'ytcfg\.set\s*\(\s*({.+?})\s*\)\s*;'
YT_INITIAL_DATA_RE = r'(?:window\s*\[\s*["\']ytInitialData["\']\s*\]|ytInitialData)\s*=\s*({.+?})\s*;\s*(?:var\s+meta|</script|\n)'


def regex_search(text, pattern, group=1, default=None):
    match = re.search(pattern, text)
    return match.group(group) if match else default


def ajax_request(session, endpoint, ytcfg, retries=5, sleep=20):
    url = 'https://www.youtube.com' + endpoint['commandMetadata']['webCommandMetadata']['apiUrl']

    data = {'context': ytcfg['INNERTUBE_CONTEXT'],
            'continuation': endpoint['continuationCommand']['token']}

    for _ in range(retries):
        response = session.post(url, params={'key': ytcfg['INNERTUBE_API_KEY']}, json=data)
        if response.status_code == 200:
            return response.json()
        if response.status_code in [403, 413]:
            return {}
        else:
            time.sleep(sleep)


def download_comments(YOUTUBE_VIDEO_URL, sort_by=SORT_BY_RECENT, language=None, sleep=0.1):
    session = requests.Session()
    session.headers['User-Agent'] = USER_AGENT
    response = session.get(YOUTUBE_VIDEO_URL)

    if 'uxe=' in response.request.url:
        session.cookies.set('CONSENT', 'YES+cb', domain='.youtube.com')
        response = session.get(YOUTUBE_VIDEO_URL)

    html = response.text
    ytcfg = json.loads(regex_search(html, YT_CFG_RE, default=''))
    if not ytcfg:
        return  # Unable to extract configuration
    if language:
        ytcfg['INNERTUBE_CONTEXT']['client']['hl'] = language

    data = json.loads(regex_search(html, YT_INITIAL_DATA_RE, default=''))

    section = next(search_dict(data, 'itemSectionRenderer'), None)
    renderer = next(search_dict(section, 'continuationItemRenderer'), None) if section else None
    if not renderer:
        # Comments disabled?
        return

    needs_sorting = sort_by != SORT_BY_POPULAR
    continuations = [renderer['continuationEndpoint']]
    while continuations:
        continuation = continuations.pop()
        response = ajax_request(session, continuation, ytcfg)

        if not response:
            break
        if list(search_dict(response, 'externalErrorMessage')):
            raise RuntimeError('Error returned from server: ' + next(search_dict(response, 'externalErrorMessage')))

        if needs_sorting:
            sort_menu = next(search_dict(response, 'sortFilterSubMenuRenderer'), {}).get('subMenuItems', [])
            if sort_by < len(sort_menu):
                continuations = [sort_menu[sort_by]['serviceEndpoint']]
                needs_sorting = False
                continue
            raise RuntimeError('Failed to set sorting')

        actions = list(search_dict(response, 'reloadContinuationItemsCommand')) + \
                  list(search_dict(response, 'appendContinuationItemsAction'))
        for action in actions:
            for item in action.get('continuationItems', []):
                if action['targetId'] == 'comments-section':
                    # Process continuations for comments and replies.
                    continuations[:0] = [ep for ep in search_dict(item, 'continuationEndpoint')]
                if action['targetId'].startswith('comment-replies-item') and 'continuationItemRenderer' in item:
                    # Process the 'Show more replies' button
                    continuations.append(next(search_dict(item, 'buttonRenderer'))['command'])

        for comment in reversed(list(search_dict(response, 'commentRenderer'))):
            yield {
                   'text': ''.join([c['text'] for c in comment['contentText'].get('runs', [])])
                   }

        time.sleep(sleep)


def search_dict(partial, search_key):
    stack = [partial]
    while stack:
        current_item = stack.pop()
        if isinstance(current_item, dict):
            for key, value in current_item.items():
                if key == search_key:
                    yield value
                else:
                    stack.append(value)
        elif isinstance(current_item, list):
            for value in current_item:
                stack.append(value)


def youtube_get(url):
    global df_comment
    df_comment = pd.DataFrame()
    
    try:
        youtube_url = url
        limit = int(entry_1.get())

        print('Downloading Youtube comments for video:', youtube_url)

        count = 0

        start_time = time.time()

        for comment in download_comments(youtube_url):

            df_comment = df_comment.append(comment, ignore_index=True)

            # comments overview
            # comment_json = json.dumps(comment, ensure_ascii=False)
            # print(comment_json)

            count += 1

            if limit and count >= limit:
                break

        print("DataFrame Shape: ", df_comment.shape, "\nComment DataFrame: ", df_comment)

        if not os.path.isfile(FILE_NAME):
            df_comment.to_csv(FILE_NAME, encoding='utf-8', index=False)
        else:  # else it exists so append without writing the header
            df_comment.to_csv(FILE_NAME, mode='a', encoding='utf-8', index=False, header=False)

        print('\n[{:.2f} seconds] Done!'.format(time.time() - start_time))

    except Exception as e:
        print('Error:', str(e))
        sys.exit(1)


#START OF THE PANEL - Codes
window = Tk()

window.title('SAGA: Youtube')

window.geometry("1366x768")
window.configure(bg = "#F4F5F9")

#to center the tab
width_of_window = 1366
height_of_window = 768
screen_width = window.winfo_screenwidth()
screen_height = window.winfo_screenheight()
x_coordinate = (screen_width/2)-(width_of_window/2)
y_coordinate = (screen_height/2)-(height_of_window/2)
window.geometry("%dx%d+%d+%d" %(width_of_window,height_of_window,x_coordinate,y_coordinate))

#The csv file might contain very huge fields, therefore increase the field_size_limit
maxInt = sys.maxsize

while True:
    try:
        csv.field_size_limit(maxInt)
        break
    except OverflowError:
        maxInt = int(maxInt/10)


canvas = Canvas(
    window,
    bg = "#F4F5F9",
    height = 768,
    width = 1366,
    bd = 0,
    highlightthickness = 0,
    scrollregion=(0,0,0,800)   #set the height for scrollbar
)

canvas.pack()

canvas.place(x = 0, y = 0)

image_image_1 = PhotoImage(
    file=relative_to_assets("image_1_yt.png"))
image_1 = canvas.create_image(
    233.0,
    59.0,
    image=image_image_1
)

image_image_2 = PhotoImage(
    file=relative_to_assets("image_2_fb.png"))
image_2 = canvas.create_image(
    126.0,
    104.0,
    image=image_image_2
)

image_image_3 = PhotoImage(
    file=relative_to_assets("image_3_fb.png"))
image_3 = canvas.create_image(
    596.0,
    155.0,
    image=image_image_3
)

image_image_4 = PhotoImage(
    file=relative_to_assets("image_5.png"))
image_4 = canvas.create_image(
    218.0,
    634.0,
    image=image_image_4
)


image_image_5 = PhotoImage(
    file=relative_to_assets("image_5_fb.png"))
image_5 = canvas.create_image(
    120.0,
    224.0,
    image=image_image_5
)


image_image_6 = PhotoImage(
    file=relative_to_assets("image_6_fb.png"))
image_6 = canvas.create_image(
    502.0,
    224.0,
    image=image_image_6
)

image_image_7 = PhotoImage(
    file=relative_to_assets("image_7_fb.png"))
image_7 = canvas.create_image(
    127.0,
    495.0,
    image=image_image_7
)

image_image_8 = PhotoImage(
    file=relative_to_assets("image_8_fb.png"))
image_8 = canvas.create_image(
    1020.0,
    224.0,
    image=image_image_8
)

entry_image_1 = PhotoImage(
    file=relative_to_assets("entry_1_fb.png"))
entry_bg_1 = canvas.create_image(
    136.0,
    155.0,
    image=entry_image_1
)


entrybox1 = StringVar()


entry_1 = Entry(
    canvas,
    bd=0,
    bg="#FFFFFF",
    highlightthickness=0,
    width=10,
    textvariable=entrybox1,
    font=("Calibri",12, 'bold')
)

entry_1.pack()
canvas.create_window(136, 155,  anchor=CENTER, window=entry_1)


canvas.create_rectangle(
    78.0,
    244.0,
    358.0,
    477.0,
    fill="#FFFFFF",
    outline="")

canvas.create_rectangle(
    80.0,
    508.0,
    356.0,
    569.0,
    fill="#FFFFFF",
    outline="")

sent_label = canvas.create_text(
    121.0,
    510.0,   #547
    anchor="nw",
    text="0",
    fill="#B94F4F",
    font=("Calibri", 45 * -1)
)

image_image_9 = PhotoImage(
    file=relative_to_assets("image_10.png"))
image_9 = canvas.create_image(
    260.0,
    540.0,
    image=image_image_9
)

canvas.create_rectangle(
    377.0,
    244.0,
    945.0,
    693.0,
    fill="#FFFFFF",
    outline="")

canvas.create_rectangle(
    964.0,
    244.0,
    1309.0,
    641.0,
    fill="#FFFFFF",
    outline="")

entry_image_2 = PhotoImage(
    file=relative_to_assets("entry_2_fb.png"))
entry_bg_2 = canvas.create_image(
    694.0,
    61.0,
    image=entry_image_2
)

entrybox2 = StringVar()
entry_2 = Entry(
    canvas,
    bd=0,
    bg="#FFFFFF",
    highlightthickness=0,
    width=65,
    textvariable = entrybox2,
    font=("Calibri",12, 'bold')
)

entry_2.pack()
canvas.create_window(694, 61,  anchor=CENTER, window=entry_2)

#code for input validation and labels
entry_1.insert(0, "Count Limit")
entry_2.insert(0, "Insert Youtube Link")

#Code for radio buttons and selecting what Datasets to use

v0 = IntVar()
v0.set(1)

r1 = Radiobutton (canvas, variable = v0, value = 1, bg = 'white', activebackground = 'white')
r2 = Radiobutton (canvas, variable = v0, value = 2, bg = 'white', activebackground = 'white')
r3 = Radiobutton (canvas, variable = v0, value = 3, bg = 'white', activebackground = 'white')

#Code for radio buttons to stick(be part) to canvas
canvas.create_window(595, 144,  anchor=NW, window=r1)
canvas.create_window(690, 144,  anchor=NW, window=r2)
canvas.create_window(800, 144,  anchor=NW, window=r3)

def on_click(event):
    if entry_1.get() == "Count Limit":
        entry_1.delete(0, END)
    else:
        print("Something is wrong in def_onclick")

    if len(entry_2.get()) == 0:
        entry_2.insert(0, "Insert Youtube Link")


def on_click2(event):
    if entry_2.get() == "Insert Youtube Link":
        entry_2.delete(0, END)
    else:
        print("Something is wrong in def_onclick2")

    if len(entry_1.get()) == 0:
        entry_1.insert(0, "Count Limit")

entry_1.bind("<Button-1>", on_click)
entry_2.bind("<Button-1>", on_click2)

####
#entry_1 is for limit and entry 2 is for link
dontupdate = "dontupdate"



def retrieve_input():
    
    
    global yt_cleanedlist
    yt_cleanedlist = []
    
    link = entry_2.get()
    limit = entry_1.get()

    #this is for dataset selection
    selection = v0.get()

    if selection == 1:
        SDD = "PoliticsDataSet"
    elif selection == 2:
        SDD = "BusinessDataSet"
    elif selection == 3:
        SDD = "EntertainmentDataSet"

    global respondsearch
    global SelectedDataSet
    SelectedDataSet = SDD + ".csv"

    try:

        if link == 'Insert Youtube Link':
            messagebox.showinfo('SAGA', 'Input keyword you want to search')
            respondsearch = 'no'
        elif limit == 'Count Limit':
            messagebox.showinfo('SAGA', 'Input how many sentiments you want to gather.')
            respondsearch = 'no'
        elif limit.isnumeric() == False:
            messagebox.showinfo('SAGA', 'Ensure that the limit is an integer.')
            respondsearch = 'no'
        else:
            limit_int = (int(limit))
            respondsearch = messagebox.askquestion('SAGA', 'Keyword: ' + link + '\n'
                                                    + 'Count: ' + str(limit_int) + '\n'
                                                    + 'Selected DataSet: ' + SelectedDataSet + '\n' + '\n'
                                                    + 'Would you like to continue?' + '\n'
                                                    )

            if respondsearch == 'yes':
                youtube_get(link)
                first_column = df_comment['text'].tolist()
                yt_retrieved = first_column
                
                print (yt_retrieved)
                for x in yt_retrieved:
                    a = re.sub("@[A-Za-z0-9_]+","",x)   #remove mentions with @
                    b = re.sub("#[A-Za-z0-9_]+","",a)   #remove hashtag
                    c = re.sub(r"\S*https?:\S*", "", b) #remove https
                    demize = emoji.demojize(c)
                    d = demize.replace(':', ' ')
                    e = d.replace("'", '') #remove hyphen
                    f = e.replace(".", '')
                    g = re.sub(r'\w*\d\w*', '', f).strip()   # remove anything with numbers
                    h = re.sub("[^a-zA-Z0-9]+", " ",g)
                    i = re.sub(r'\b\w{1,2}\b', '', h)
                    yt_cleanedlist.append(i)
                    
                for i in range(len(yt_cleanedlist)):
                    yt_cleanedlist[i] = yt_cleanedlist[i].lower()
                    
                '''   LANGUAGE DETECTION
                es_removal = 0
                   
                while es_removal < len(yt_cleanedlist):
                    if (detect(yt_cleanedlist[es_removal])) == 'en':
                        print ("THIS 2 fan")
                        #print ('2 ' +detect(yt_cleanedlist[es_removal]))
                        #yt_cleanedlist.remove(yt_cleanedlist[es_removal])
                        es_removal +=1
                    else:
                        es_removal +=1
                        print ('1 ' + detect(yt_cleanedlist[es_removal]))            
                '''    
                
                header = ['Documents']
                with open('CollectedYtComments.csv','w', encoding='UTF8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(header)
                    for word in range(len(yt_cleanedlist)):
                        writer.writerow([yt_cleanedlist[word]])
                   
                global dontupdate
                dontupdate = "plsupdate"
                
            elif respondsearch == 'no':
                return None
            else:
                messagebox.showwarning('Error', 'Something went wrong')
    
    
    except Exception as e:
        messagebox.showerror( "Error","No Sentiments gathered,\nensure that all parameters are correct")
        respondsearch = 'no'

def calculating_naive():

    DataSetUse = SelectedDataSet
    #this is the first step, list for the positive, negative, and neutral sentiment
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    #this will count all the rows of positive and negative
    with open(DataSetUse, 'r', encoding="UTF8") as csv_file:
        csv_read = csv.reader(csv_file, delimiter=',')
        for row in csv_read:
            if "positive" in row[1]:
                positive_count+=1
            elif "negative" in row[1]:
                negative_count+=1
            elif "neutral" in row[1]:
                neutral_count+=1

    sentiment_total = positive_count + negative_count + neutral_count


    #end of first Step
    words= []

    with open( DataSetUse, 'r', encoding="UTF8") as csv_file:
        reader = csv.reader(csv_file, delimiter=',')
        for row in reader:
            if row[0] != " ":                            #remove the header, what to do?
                csv_words = row[0].split(" ")
                for x in csv_words:
                    words.append(x)

    #this one below, I dont know what to do yet.
    words_counted = []
    for i in words:
        x = words.count(i)
        words_counted.append((i,x))

    #Convert List to Dictionary to avoid duplication of word count or you can just use set.
    def Convert(words_counted):
        res_dct = {words_counted[i]: words_counted[i + 1] for i in range(0, len(words_counted), 2)}
        return res_dct


    # trying to do the 2nd Step for finding the total words with positive sentiment
    positive_word = []  #list for all the positive, negative, and neutral word in the dataset
    negative_word = []
    neutral_word = []

    with open( DataSetUse, 'r', encoding="UTF8") as csv_file:
        csv_read = csv.reader(csv_file, delimiter=',')
        for row in csv_read:
            if "positive" in row[1]:
                csv_words = row[0].split(" ")
                for x in csv_words:
                    positive_word.append(x)
            elif "negative" in row[1]:
                csv_words = row[0].split(" ")
                for x in csv_words:
                    negative_word.append(x)
            elif "neutral" in row[1]:
                csv_words = row[0].split(" ")
                for x in csv_words:
                    neutral_word.append(x)

    print("Fantastic")

    final_positive = set(positive_word)  #total count of positive words in the dataset without duplicate
    final_negative = set(negative_word) #total count of negative words in the dataset without duplicate
    final_neutral = set(neutral_word) #total count of negative words in the dataset without duplicate

    merging_words = positive_word + negative_word + neutral_word   # need this for overall words as a denominator in the formula
    merging_words1 = set(merging_words)

    #code for opening the CSV File collected tweets

    global sentiment_final
    sentiment_final = []
    testing = []
    with open('CollectedYtComments.csv', 'r', encoding="UTF8") as csv_file:
        csv_read = csv.reader(csv_file, delimiter=',')
        for row in csv_read:
            if row[0] == "Documents":
                print ("Hello")
            elif row[0] == " ":
                print("hello2")
            elif row[0] != "Documents" and " ":
                testing.append(row[0])
                

# this analyzes the content of the CSV File but one 1 at at time

                document_splitted_original = []
                document_splitted = []
                counting_posword_list = []
                counting_negaword_list = []
                counting_neutword_list = []

    #computing for positive, splitting the sentence with comma then adding it to the list
                for cd in testing:
                    testing_word = testing[0].split(" ")  #testing[0], [0] means the column index of the list
                    for ef in testing_word:
                        document_splitted_original.append(ef)
                for one in document_splitted_original:
                    if one.strip() != '':
                        document_splitted.append(one)

#removing the words not in the training dataset, this cover the removal among negative, positive, and neutral
                index_removal = 0

                while index_removal < len(document_splitted):
                    if document_splitted[index_removal] not in merging_words:
                        document_splitted.remove(document_splitted[index_removal])
                        index_removal +=1
                    else:
                        index_removal +=1

                counter_for_words = 0
                index_counter = 0

                while counter_for_words < len(document_splitted):
                #for positive
                    if document_splitted[index_counter] in positive_word:
                        counting_pword = positive_word.count(document_splitted[index_counter])
                        counting_posword_list.append(counting_pword)

                    elif document_splitted[index_counter] in merging_words:
                        counting_posword_list.append(0)

                    else:
                        counting_posword_list.append(0)

                # for negative
                    if document_splitted[index_counter] in negative_word:
                        counting_negaword = negative_word.count(document_splitted[index_counter])
                        counting_negaword_list.append(counting_negaword)

                    elif document_splitted[index_counter] in merging_words:
                        counting_negaword_list.append(0)

                    else:
                        counting_negaword_list.append(0)


                #for neutral
                    if document_splitted[index_counter] in neutral_word:
                        counting_neutword = neutral_word.count(document_splitted[index_counter])
                        counting_neutword_list.append(counting_neutword)

                    elif document_splitted[index_counter] in merging_words:
                        counting_neutword_list.append(0)

                    else:
                        counting_neutword_list.append(0)

                    counter_for_words+=1
                    index_counter+=1


    #computation let's go
                #for positive computation
                positive_with_one = []
                result_positive = 0
                for parampos in counting_posword_list:
                    result_positive = parampos + 1
                    positive_with_one.append(result_positive)

                positive_naive = (positive_count / sentiment_total) * (numpy.prod(positive_with_one)) / ((len(positive_word) + len(merging_words1))**(len(positive_with_one)))


                #for negative computation
                negative_with_one = []
                result_negative = 0
                for paramneg in counting_negaword_list:
                    result_negative = paramneg + 1
                    negative_with_one.append(result_negative)

                negative_naive = (negative_count / sentiment_total) * (numpy.prod(negative_with_one)) / ((len(negative_word) + len(merging_words1))**(len(negative_with_one)))

                #for neutral computation
                neutral_with_one = []
                result_neutral = 0
                for paramneut in counting_neutword_list:
                    result_neutral = paramneut + 1
                    neutral_with_one.append(result_neutral)

                neutral_naive = (neutral_count / sentiment_total) * (numpy.prod(neutral_with_one)) / ((len(neutral_word) + len(merging_words1))**(len(neutral_with_one)))


                if positive_naive > (negative_naive and neutral_naive):
                    sentiment_final.append("positive")
                elif negative_naive > (positive_naive and neutral_naive):
                    sentiment_final.append("negative")
                elif neutral_naive > (positive_naive and negative_naive):
                    sentiment_final.append("neutral")
                else:
                    print("there might be a problem3")

                testing.clear()
                document_splitted.clear()
                counting_posword_list.clear()
                counting_negaword_list.clear()
                counting_neutword_list.clear()
                document_splitted_original.clear()

                index_removal = 0
                counter_for_words = 0
                index_counter = 0
                result_positive = 0
                result_neutral = 0
                result_negative = 0
                positive_with_one.clear()
                negative_with_one.clear()
                neutral_with_one.clear()

    return sentiment_final

#wordcloud code  -----WORDCLOUD-----
#getting the info from the my_list then saving it as png file, then calling ito to display the png

def countingcollectedfbcomments():

    sentiments_gathered = 0 - 1
    #this for sentiments gatheres
    with open('CollectedYtComments.csv', 'r', encoding="UTF8") as csv_file:
        reader = csv.reader(csv_file, delimiter=',')
        for row in reader:
            sentiments_gathered += 1

    canvas.itemconfig(sent_label, text=str(sentiments_gathered))

def piechart(sentiment_final):
    # code for pie chart   ----PIE CHART

    pie_chart = []
    pieexplode = [0.03, 0.03, 0.03]
    piecolor = ["#00AF50", "#BF0002", "#FECE0A"]

    positive_forpie = sentiment_final.count('positive')
    negative_forpie = sentiment_final.count('negative')
    neutral_forpie = sentiment_final.count('neutral')



    pie_chart.append(positive_forpie)
    pie_chart.append(negative_forpie)
    pie_chart.append(neutral_forpie)

    positive_pircent = (positive_forpie / len(sentiment_final)) * 100
    negative_pircent = (negative_forpie / len(sentiment_final)) * 100
    neutral_pircent = (neutral_forpie / len(sentiment_final)) * 100

    positive_pircent = str(round(positive_pircent, 2))
    negative_pircent = str(round(negative_pircent , 2))
    neutral_pircent = str(round(neutral_pircent , 2))

    PieLabels = ['POS: ' + positive_pircent, 'N: ' + negative_pircent, 'NEU: ' + neutral_pircent]

    #create a new frame for pie charts
    frm = tkinter.Frame(window)

    fig = matplotlib.figure.Figure(figsize=(2.8,2.3))
    ax = fig.add_subplot(111) # add an Axes to the figure

    pie = ax.pie(pie_chart, radius=1, explode= pieexplode, colors = piecolor)

    circle=matplotlib.patches.Circle( (0,0), 0.7, color='white')
    ax.add_artist(circle)

    ax2 = fig.add_subplot(212)
    ax2.axis("off")
    ax2.legend(pie[0],PieLabels, loc="lower right")

    chart1 = FigureCanvasTkAgg(fig, master=frm)
    chart1.get_tk_widget().pack()

    canvas.create_window(77, 240,  anchor=NW, window=frm)



def appendingsentiment(sentiment_final):

    with open('CollectedYtComments.csv', 'r') as read_object, \
        open('YtWithSentiment.csv', 'w', newline='') as write_object:
        csv_reader = reader(read_object)

        csv_writer = writer(write_object)

        row = next(csv_reader)
        row.append("Sentiment")
        csv_writer.writerow(row)

        it = sentiment_final.__iter__()

        for row in csv_reader:
            if row:
                try:
                    row.append(next(it))
                except StopIteration:
                    row.append("N/A")
                csv_writer.writerow(row)


def showingtable():
    # Code for tabular form

    TableMargin = Frame(canvas, width=568, height=500)
    TableMargin.pack()
    canvas.create_window(377, 240,  anchor=NW, window=TableMargin)

    scrollbarx = Scrollbar(TableMargin, orient=HORIZONTAL)
    scrollbary = Scrollbar(TableMargin, orient=VERTICAL)

    tree = ttk.Treeview (
        TableMargin,
        columns=("Documents", "Sentiments"),
        height=21,
        selectmode="extended",
        yscrollcommand=scrollbary.set,
        xscrollcommand=scrollbarx.set,
        )

    scrollbary.config(command=tree.yview)
    scrollbary.pack(side=RIGHT, fill=Y)
    scrollbarx.config(command=tree.xview)
    scrollbarx.pack(side=BOTTOM, fill=X)

    tree.heading('Documents', text="Documents", anchor=CENTER)
    tree.heading('Sentiments', text="Sentiments", anchor=CENTER)

    tree.column('#0', stretch=FALSE, minwidth=0, width=0, anchor=W)
    tree.column('#1', stretch=FALSE, minwidth=20, width=400, anchor=W)
    tree.column('#2', stretch=FALSE, minwidth=20, width=150, anchor=W)
    tree.pack(expand=True,fill=BOTH)

    #code to open the CSV na na-analyzed na
    with open('YtWithSentiment.csv') as f:
        reader = csv.DictReader(f, delimiter=',')
        for row in reader:

            Documents = row['Documents']
            Sentiments = row['Sentiment']

            tree.insert("", 1, values=(Documents, Sentiments))
    #End of table form code

def wordcloud():
    words_collectedfbcomments = []

    with open('CollectedYtComments.csv', 'r', encoding="UTF8") as csv_file:
        reader = csv.reader(csv_file, delimiter=',')
        for row in reader:
            if row[0] != " ":                            #remove the header, what to do?
                csv_words = row[0].split(" ")
                for x in csv_words:
                    words_collectedfbcomments.append(x)


    unique_string=(" ").join(words_collectedfbcomments)
    wc = WordCloud(background_color="white", width=345, height=397).generate(unique_string)

    plt.imshow(wc)
    plt.axis("off")
    plt.savefig("wordcloud_output"+".png", bbox_inches='tight')


    #inserting image, this is the wordcloud
    img = ImageTk.PhotoImage(Image.open("wordcloud_output.png"))

    # Create a Label Widget to display the text or Image
    imagecontainer = Label(canvas, image = img)
    imagecontainer.photo=img
    imagecontainer.pack()

    canvas.create_window(964, 240,  anchor=NW, window=imagecontainer)

def clearall():
    entry_1.delete(0, END)
    entry_2.delete(0, END)
    entry_1.insert(0, "Count Limit")
    entry_2.insert(0, "Insert Youtube Link")
    v0.set(1)

def SaveTo():

    if dontupdate == "dontupdate":
        messagebox.showinfo("SAGA", "No CSV File to download")
    else:
        collectedfb_csv = yt_cleanedlist
        categoriesfb_csv = sentiment_final

        data = [("Comma Separated Value",".csv")]
        file = filedialog.asksaveasfilename(filetypes = data, defaultextension = '.csv')

        if file:
            header = ['Documents', 'Sentiment']
            with open(file,"w", encoding='UTF8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(header)

                for document in range(len(collectedfb_csv)):
                    writer.writerow([collectedfb_csv[document], categoriesfb_csv[document]])

            messagebox.showinfo('SAGA', 'Succesfully saved.' )
        else:
            print ("cancelled")


#code for update button
def update():


    if dontupdate == 'dontupdate':
        messagebox.showinfo("SAGA", "Nothing to Update here.")
    else:
        selection = v0.get()

        if selection == 1:
            SDD = "PoliticsDataSet"
        elif selection == 2:
            SDD = "BusinessDataSet"
        elif selection == 3:
            SDD = "EntertainmentDataSet"

        SelectedDataSet = SDD + ".csv"

        updatingdataset = messagebox.askquestion ("SAGA", "Update " + SelectedDataSet + " ?")

        if updatingdataset == "yes":

            file = SelectedDataSet
            SelectedDataSet = pd.read_csv(SelectedDataSet, encoding='latin')
            Collected = pd.read_csv ('YtWithSentiment.csv', encoding='latin')
            pd.concat([SelectedDataSet, Collected]).to_csv(file, index=False, header=True)

            RemoveDuplicate = pd.read_csv(file)
            RemoveDuplicate[~RemoveDuplicate.duplicated(subset=['Documents'])].to_csv(file, index=False)
            print("Possible duplicates removed.")

            messagebox.showinfo('SAGA', file + ' was succesfully updated \nand possible duplicates are removed.' )

        else:
            print ("EDI DON'T")

def main():
    retrieve_input()
    if respondsearch == 'yes':
        calculating_naive()
        wordcloud()
        sentiment_final = calculating_naive()
        piechart(sentiment_final)
        countingcollectedfbcomments()
        appendingsentiment(sentiment_final)
        showingtable()
    elif respondsearch == 'no':
        print ("OK1")
    else:
        print ("PROBLEN IN MAIN")
    

def backtodash():
    window.destroy()
    import Dashboard

#button for back to dahboard
back_button_img = PhotoImage(
    file=relative_to_assets("back_btn.png"))

back_button = Button(
    image=back_button_img,
    borderwidth=0,
    highlightthickness=0,
    command=lambda:[backtodash()],
    relief="flat"
)

back_button.place(
    x=1107.0,
    y=43.0,
    width=202.0,
    height=35.0
)

canvas.create_window(1107, 43,  anchor=NW, window=back_button)

#button for clear all
button_image_1 = PhotoImage(
    file=relative_to_assets("button_1.png"))

button_1 = Button(
    image=button_image_1,
    borderwidth=0,
    highlightthickness=0,
    command=lambda: [clearall()],
    relief="flat"
)

button_1.place(
    x=990.0,
    y=137.0,
    width=105.0,
    height=35.0
)

canvas.create_window(990, 137,  anchor=NW, window=button_1)

#button for download
button_image_2 = PhotoImage(
    file=relative_to_assets("button_2_fb.png"))
button_2 = Button(
    image=button_image_2,
    borderwidth=0,
    highlightthickness=0,
    command=lambda:[SaveTo()],
    relief="flat"
)

button_2.place(
    x=1143.0,
    y=654.0,
    width=151.0,
    height=36.0
)

canvas.create_window(1143, 654,  anchor=NW, window=button_2)

button_image_3 = PhotoImage(
    file=relative_to_assets("button_3.png"))
button_3 = Button(
    image=button_image_3,
    borderwidth=0,
    highlightthickness=0,
    command=lambda:[update()],
    relief="flat"
)
button_3.place(
    x=980.0,
    y=654.0,
    width=151.0,
    height=36.0
)

canvas.create_window(980, 654,  anchor=NW, window=button_3)

#search and analyze button
button_image_4 = PhotoImage(
    file=relative_to_assets("button_4_fb.png"))
button_4 = Button(
    image=button_image_4,
    borderwidth=0,
    highlightthickness=0,
    command=lambda:[main()],
    relief="flat"
)
button_4.place(
    x=1107.0,
    y=137.0,
    width=202.0,
    height=35.0
)

canvas.create_window(1107, 137,  anchor=NW, window=button_4)




window.resizable(False, False)

#Code for ScrollBar

vbar=Scrollbar(canvas,orient=VERTICAL)
vbar.pack(side=RIGHT,fill=Y)
vbar.config(command=canvas.yview)
canvas.config(width=1366,height=768)
canvas.config(yscrollcommand=vbar.set)
canvas.pack(side=LEFT,expand=True,fill=BOTH)



window.mainloop()
