PREPROCESS.PY
#Reading CSV File, Inserting to List, Cleaning the Document
def clean(csvfile):
import csv
import re #for cleaning
import emoji #for demojinize
FILE_DIR = csvfile
#open the csv file, if charmap error exist -> put encoding="utf8"
with open(FILE_DIR, newline='', encoding="utf8") as f:
 reader = csv.reader(f)
 document = list(reader)
 f.close()

stopword_en = 'authentication\stopwords-en.txt'
with open(stopword_en, 'r', encoding="utf-8") as enfile:
 stopwordsliste = [enline.strip() for enline in enfile]
 enfile.close()
stopword_tl = 'authentication\stopwords-tl.txt'
with open(stopword_tl, 'r', encoding="utf-8") as tlfile:
 stopwordslistl = [tlline.strip() for tlline in tlfile]
 tlfile.close()

'''
tagalog_dict = 'authentication\dictionary-tl.txt'
with open(tagalog_dict, 'r', encoding="utf-8") as tldict:
 tldict_list = [dline.strip() for dline in tldict]
 tldict.close()
'''
english_dict = 'authentication\en-dict.txt'
with open(english_dict, 'r', encoding="utf-8") as endict:
 endict_list = [eline.strip() for eline in endict]
 endict.close()

l_cleaned = []
for bracket in document:
 for content in bracket:

 #cleaning contractions
 from contraction import contractions
 for _word in content.split():
 if _word.lower() in contractions:
 content = content.replace(_word, contractions[_word.lower()])

 #converting the txt into dictionary
 dicti={}

f = open("authentication\stemming-w.txt", "r").read().splitlines()
 for x in f:
 dicti[x.split(' ')[0]]=x.split(' ',maxsplit=1)[1]

 for _xword in content.split():
 if _xword in dicti:
 content = content.replace(_xword, dicti[_xword])

 _a = re.sub(r'http\S+', ' ', content) #remove http
 _b = re.sub(r'www\S+', ' ', _a) #remove links
 _c = re.sub('#[A-Za-z0-9_]+', ' ',_b) #remove hashtags
 _d = re.sub("@[A-Za-z0-9_]+", ' ',_c) #remove words that starts with @
 _e = emoji.demojize(_d) #convert emojis into words
 _f = re.sub('[^A-Za-z ]+', ' ', _e) #Remove all non-alpha characters
 _g = re.sub(r'\b\w{1,3}\b', '', _f) #remove less than 4 characters
 _h = _g.lower() #convert uppercase to lowercase
 _i = ' '.join(_h.split()) #handles multiple spaces

 #remove repetition of word in same sentence, and handle order
 seen = set()
 result = []
 for item in _i.split():
 if item not in seen:
 seen.add(item)
 result.append(item)

#remove stopwords
 wordc = []
 for enword in result[:]:
 if enword in stopwordsliste:
 result.remove(enword)
 if enword in stopwordslistl:
 result.remove(enword)
 if enword in endict_list: #remove words not in dictionary
 wordc.append(enword)
 _stoplist = ' '.join(wordc)

 l_cleaned.append(_stoplist)
 l_cleaned = [x for x in l_cleaned if x] #remove empty list
 l_cleaned = [x.strip(' ') for x in l_cleaned] #remove spaces between
xseen = set()
xresult = []
for item in l_cleaned:
 if item not in xseen:
 xseen.add(item)
 xresult.append(item)
import sqlite3

#define connection and cursor
conn = sqlite3.connect('databases/sentiment.db')
c = conn.cursor()
 c.execute("""CREATE TABLE IF NOT EXISTS document
 ( comment TEXT, category TEXT)""")
#delete all the content of the database table document, for fresh start.
 c.execute("DELETE FROM document")
#insert links into table
def data_entry():
 for item in xresult:
 c.execute("INSERT INTO document(comment) VALUES (?)", (item,))
 conn.commit()
data_entry()
conn.close()

ANALYZER.PY
#Guide from https://stackoverflow.com/questions/29582736/python3-is-there-a-way-toiterate-row-by-row-over-a-very-large-sqlite-table-wi
#Guide from https://stackoverflow.com/questions/2854011/get-a-list-of-field-valuesfrom-pythons-sqlite3-not-tuples-representing-rows
def calc_dataset(selectedset):
import sqlite3
import numpy
#define connection and cursor
conn = sqlite3.connect('databases/'+selectedset+'.db')
c = conn.cursor()
conn_two = sqlite3.connect('databases/sentiment.db')
c_two = conn_two.cursor()

#count the category in the dataset
ps_count = 0
ng_count = 0
nt_count = 0
for category in c.execute("SELECT category FROM document"):
 if 'positive' in category:
 ps_count += 1
 elif 'negative' in category:
 ng_count += 1

elif 'neutral' in category:
 nt_count += 1
ds_total = ps_count + ng_count + nt_count
words_per_row = []
#get all the document per row list and split it into unigram
for row in c.execute("SELECT comment FROM document"):
 for words in row:
 splitted_word = words.split()
 for x in splitted_word:
 words_per_row.append(x)
#count words occurrences
words_occurrence = {}
for x in words_per_row:
 y = words_per_row.count(x)
 words_occurrence[x] = y
#count total words of every category
ps_wordcount = []
ng_wordcount = []
nt_wordcount = []
merge_category = ps_wordcount + ng_wordcount + nt_wordcount
for row in c.execute("SELECT * FROM document"):

if 'positive' in row[1]:
 splitted_word = row[0].split()
 for pword in splitted_word:
 ps_wordcount.append(pword)
 elif 'negative' in row[1]:
 splitted_word = row[0].split()
 for ngword in splitted_word:
 ng_wordcount.append(ngword)
 elif 'neutral' in row[1]:
 splitted_word = row[0].split()
 for ntword in splitted_word:
 nt_wordcount.append(ntword)

#convert list to dictionary
merge_category = [*set(ps_wordcount + ng_wordcount + nt_wordcount)]
document_get = []
splitted_doc = []
#starts for analyzing the inputted
 for row in c_two.execute("SELECT comment FROM document"):
 document_get.append(row)
#for unigram?
for x in document_get:

splitted_word = x[0].split()
 for y in splitted_word:
 splitted_doc.append(y)

 identifier = ' '.join(splitted_doc) #this one, identify the column for database
insertion

 dropcounter = 0
 #removing the words not found in dataset...
 while dropcounter < (len(splitted_doc)):
 if splitted_doc[dropcounter] not in merge_category:
 splitted_doc.remove(splitted_doc[dropcounter])
 dropcounter += 1
 else:
 dropcounter += 1

 wordcounter = 0
 indexcounter = 0
 pword_fromtest = []
 ngword_fromtest = []
 ntword_fromtest = []

 while wordcounter < len(splitted_doc):
 #for positive
 if splitted_doc[indexcounter] in ps_wordcount:
 count_pword = words_occurrence[splitted_doc[indexcounter]]

pword_fromtest.append(count_pword + 1)
 else:
 pword_fromtest.append(1)

 #for negative
 if splitted_doc[indexcounter] in ng_wordcount:
 count_ngword = words_occurrence[splitted_doc[indexcounter]]
 ngword_fromtest.append(count_ngword + 1)
 else:
 ngword_fromtest.append(1)

 #for neutral
 if splitted_doc[indexcounter] in nt_wordcount:
 count_ntword = words_occurrence[splitted_doc[indexcounter]]
 ntword_fromtest.append(count_ntword + 1)
 else:
 ntword_fromtest.append(1)
 wordcounter += 1
 indexcounter += 1
 #computation
 p_naive = ((ps_count / ds_total) * (numpy.prod(pword_fromtest))
 / ((len(ps_wordcount) + len(merge_category))**(len(splitted_doc))))

 ng_naive = ((ng_count / d

/ ((len(ng_wordcount) + len(merge_category))**(len(splitted_doc))))

 nt_naive = ((nt_count / ds_total) * (numpy.prod(ntword_fromtest))
 / ((len(nt_wordcount) + len(merge_category))**(len(splitted_doc))))
 if (p_naive > ng_naive) and (p_naive > nt_naive):
 c_two.execute("""UPDATE document SET category=? WHERE comment = ?""",
('positive', identifier,))
 elif (ng_naive > p_naive) and (ng_naive > nt_naive):
 c_two.execute("""UPDATE document SET category=? WHERE comment = ?""",
('negative', identifier,))
 elif (nt_naive > p_naive) and (nt_naive > ng_naive):
 c_two.execute("""UPDATE document SET category=? WHERE comment=?""",
('neutral', identifier))
 else:
 None
 dropcounter = 0
 wordcounter = 0
 indexcounter = 0
 splitted_doc.clear()
 pword_fromtest.clear()
 ngword_fromtest.clear()
 ntword_fromtest.clear()
 conn_two.commit()
 conn_two.close()
 conn.close()
